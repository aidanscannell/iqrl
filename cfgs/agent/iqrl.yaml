defaults:
  - base_iqrl

##### General #####
mlp_dims: [512, 512]
lr: 3e-4
batch_size: 256
utd_ratio: 1  # parameter update-to-data ratio
actor_update_freq: 2  # update actor less frequently than critic
gamma: 0.99  # discount factor
tau: 0.005 # target network update rate
nstep: 1  # nstep returns

##### Encoder config #####
latent_dim: 512
horizon: 5  # horizon used for representation learning
rho: 0.9  # discount factor for dynamics
enc_mlp_dims: [256]
enc_lr: 1e-4
enc_tau: 0.005
enc_update_freq: 1  # update encoder less frequently than actor/critic
grad_clip_norm: 20
use_tar_enc: True # if True use a target encoder

##### Configure which loss terms to use #####
use_tc_loss: True # if True include dynamic model in encoder
use_rew_loss: False # if True use reward prediction for representation learning
use_cosine_similarity_dynamics: True

##### FSQ normalization config #####
use_fsq: True
fsq_levels: [8, 8]

##### (Optionally) Project loss into another space before calculating TC loss #####
use_latent_projection: False
projection_mlp_dims: [256]
proj_dim: null # if null proj_dim=int(latent_dim/16)

##### Exploration noise schedule #####
exploration_noise_start: 1.0
exploration_noise_end: 0.1
exploration_noise_num_steps: 50  # number of episodes do decay noise

##### Policy smoothing #####
policy_noise: 0.2
noise_clip: 0.3

##### Other stuff #####
logging_freq: 100
compile: False
device: ${device}
name: "iQRL"
